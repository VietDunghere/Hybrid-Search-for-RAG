"""
Module Ä‘Ã¡nh giÃ¡: sá»­ dá»¥ng RAGAS framework Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng RAG pipeline
CÃ¡c metrics: Faithfulness, Answer Relevancy, Context Precision, Context Recall
"""
import json
import os
from typing import List, Dict

from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from config import (
    LLM_API_KEY, LLM_BASE_URL, LLM_MODEL, LLM_TEMPERATURE,
    EVAL_FILE, OUTPUT_DIR, EMBEDDING_MODEL
)


def create_ragas_llm():
    """
    Táº¡o LLM wrapper cho RAGAS sá»­ dá»¥ng NVIDIA NIM (OpenAI-compatible API)
    """
    llm = ChatOpenAI(
        model=LLM_MODEL,
        api_key=LLM_API_KEY,
        base_url=LLM_BASE_URL,
        temperature=0.1,  # dÃ¹ng temperature tháº¥p cho evaluation
        max_tokens=1024,
    )
    return LangchainLLMWrapper(llm)


def create_ragas_embeddings():
    """
    Táº¡o Embedding wrapper cho RAGAS sá»­ dá»¥ng NVIDIA NIM embeddings
    """
    embeddings = OpenAIEmbeddings(
        model="nvidia/nv-embedqa-e5-v5",
        api_key=LLM_API_KEY,
        base_url=LLM_BASE_URL,
    )
    return LangchainEmbeddingsWrapper(embeddings)


def prepare_ragas_dataset(pipeline_results: List[Dict]) -> Dataset:
    """
    Chuyá»ƒn Ä‘á»•i káº¿t quáº£ tá»« RAG pipeline sang format RAGAS Dataset
    
    RAGAS yÃªu cáº§u:
    - question: cÃ¢u há»i
    - answer: cÃ¢u tráº£ lá»i tá»« LLM
    - contexts: list cÃ¡c context chunks
    - ground_truth: Ä‘Ã¡p Ã¡n mong Ä‘á»£i (cho context_recall)
    """
    questions = []
    answers = []
    contexts = []
    ground_truths = []
    
    for result in pipeline_results:
        questions.append(result["query"])
        answers.append(result["answer"])
        
        # Láº¥y content tá»« cÃ¡c chunks Ä‘Ã£ retrieve
        chunk_contents = [chunk["content"] for chunk in result["retrieved_chunks"]]
        contexts.append(chunk_contents)
        
        ground_truths.append(result.get("expected_answer", ""))
    
    data = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths,
    }
    
    return Dataset.from_dict(data)


def run_ragas_evaluation(pipeline_results: List[Dict]) -> Dict:
    """
    Cháº¡y Ä‘Ã¡nh giÃ¡ RAGAS trÃªn káº¿t quáº£ pipeline
    
    Returns:
        Dict chá»©a scores cho cÃ¡c metrics
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š Báº®T Äáº¦U ÄÃNH GIÃ Vá»šI RAGAS")
    print("=" * 60)
    
    # Chuáº©n bá»‹ dataset
    print("\nğŸ”„ Äang chuáº©n bá»‹ dataset cho RAGAS...")
    dataset = prepare_ragas_dataset(pipeline_results)
    print(f"âœ… Dataset: {len(dataset)} samples")
    
    # Táº¡o LLM vÃ  Embeddings cho RAGAS
    print("ğŸ”„ Äang khá»Ÿi táº¡o LLM vÃ  Embeddings cho RAGAS...")
    ragas_llm = create_ragas_llm()
    ragas_embeddings = create_ragas_embeddings()
    
    # Cháº¡y evaluation
    print("ğŸ”„ Äang cháº¡y evaluation (cÃ³ thá»ƒ máº¥t vÃ i phÃºt)...")
    
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ]
    
    try:
        result = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=ragas_llm,
            embeddings=ragas_embeddings,
        )
        
        print("\n" + "=" * 60)
        print("ğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ RAGAS")
        print("=" * 60)
        print(f"\n  Faithfulness:       {result['faithfulness']:.4f}")
        print(f"  Answer Relevancy:   {result['answer_relevancy']:.4f}")
        print(f"  Context Precision:  {result['context_precision']:.4f}")
        print(f"  Context Recall:     {result['context_recall']:.4f}")
        
        # TÃ­nh average score
        avg_score = (
            result['faithfulness'] + 
            result['answer_relevancy'] + 
            result['context_precision'] + 
            result['context_recall']
        ) / 4
        print(f"\n  ğŸ“ˆ Average Score:   {avg_score:.4f}")
        print("=" * 60)
        
        return {
            "faithfulness": float(result['faithfulness']),
            "answer_relevancy": float(result['answer_relevancy']),
            "context_precision": float(result['context_precision']),
            "context_recall": float(result['context_recall']),
            "average_score": float(avg_score),
        }
        
    except Exception as e:
        print(f"\nâš ï¸ Lá»—i khi cháº¡y RAGAS evaluation: {str(e)}")
        print("Äang thá»­ cháº¡y tá»«ng metric riÃªng láº»...")
        
        individual_results = {}
        for metric in metrics:
            try:
                result = evaluate(
                    dataset=dataset,
                    metrics=[metric],
                    llm=ragas_llm,
                    embeddings=ragas_embeddings,
                )
                metric_name = metric.name
                individual_results[metric_name] = float(result[metric_name])
                print(f"  âœ… {metric_name}: {result[metric_name]:.4f}")
            except Exception as me:
                metric_name = metric.name
                individual_results[metric_name] = None
                print(f"  âŒ {metric_name}: Lá»—i - {str(me)}")
        
        # TÃ­nh average cho cÃ¡c metric thÃ nh cÃ´ng
        valid_scores = [v for v in individual_results.values() if v is not None]
        if valid_scores:
            individual_results["average_score"] = sum(valid_scores) / len(valid_scores)
        else:
            individual_results["average_score"] = None
            
        return individual_results


def save_evaluation_report(
    pipeline_results: List[Dict],
    ragas_scores: Dict,
    output_dir: str = OUTPUT_DIR,
):
    """
    LÆ°u bÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡ Ä‘áº§y Ä‘á»§
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. LÆ°u káº¿t quáº£ chi tiáº¿t (JSON)
    detail_path = os.path.join(output_dir, "evaluation_details.json")
    
    # Serialize-safe results
    serializable_results = []
    for r in pipeline_results:
        sr = {
            "question_index": r.get("question_index"),
            "question_type": r.get("question_type"),
            "query": r.get("query"),
            "case_context": r.get("case_context", ""),
            "answer": r.get("answer"),
            "expected_answer": r.get("expected_answer"),
            "retrieved_chunks": [
                {
                    "chunk_id": c["chunk_id"],
                    "source": c["metadata"]["filename"],
                    "section": c["metadata"].get("section_header", ""),
                    "hybrid_score": c.get("hybrid_score", 0),
                    "bm25_score": c.get("bm25_score", 0),
                    "dense_score": c.get("dense_score", 0),
                    "content": c["content"][:500],  # Cáº¯t bá»›t Ä‘á»ƒ file khÃ´ng quÃ¡ lá»›n
                }
                for c in r.get("retrieved_chunks", [])
            ]
        }
        serializable_results.append(sr)
    
    report = {
        "ragas_scores": ragas_scores,
        "total_questions": len(pipeline_results),
        "results": serializable_results,
    }
    
    with open(detail_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ Chi tiáº¿t Ä‘Ã£ lÆ°u: {detail_path}")
    
    # 2. LÆ°u bÃ¡o cÃ¡o text
    report_path = os.path.join(output_dir, "evaluation_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("BÃO CÃO ÄÃNH GIÃ RAG PIPELINE - HYBRID SEARCH\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("ğŸ“Š RAGAS SCORES:\n")
        for metric, score in ragas_scores.items():
            if score is not None:
                f.write(f"  {metric}: {score:.4f}\n")
            else:
                f.write(f"  {metric}: N/A\n")
        
        f.write(f"\nğŸ“‹ Tá»”NG Sá» CÃ‚U Há»I: {len(pipeline_results)}\n")
        
        # Chi tiáº¿t tá»«ng cÃ¢u
        from rag_pipeline import RAGPipeline
        dummy_pipeline = type('obj', (object,), {'format_results': RAGPipeline.format_results})()
        formatted = RAGPipeline.format_results(None, pipeline_results)
        f.write("\n" + formatted)
    
    print(f"ğŸ’¾ BÃ¡o cÃ¡o Ä‘Ã£ lÆ°u: {report_path}")
    
    return detail_path, report_path


if __name__ == "__main__":
    # Module nÃ y Ä‘Æ°á»£c gá»i tá»« main.py
    print("Module evaluate - sá»­ dá»¥ng tá»« main.py")
    print("Cháº¡y: python main.py")
