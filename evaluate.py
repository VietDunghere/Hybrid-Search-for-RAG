"""
Module Ä‘Ã¡nh giÃ¡: sá»­ dá»¥ng RAGAS framework (v0.4.x) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng RAG pipeline
CÃ¡c metrics: Faithfulness, ResponseRelevancy, LLMContextPrecisionWithReference, ContextRecall
"""
import json
import os
from typing import List, Dict

from ragas import evaluate, RunConfig, EvaluationDataset, SingleTurnSample
from ragas.metrics import (
    Faithfulness,
    ResponseRelevancy,
    LLMContextPrecisionWithReference,
    ContextRecall,
)
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from config import (
    LLM_API_KEY, LLM_BASE_URL, LLM_MODEL, LLM_TEMPERATURE,
    EVAL_FILE, OUTPUT_DIR, EMBEDDING_MODEL
)


def create_ragas_llm():
    """
    Táº¡o LLM wrapper cho RAGAS sá»­ dá»¥ng NVIDIA NIM (OpenAI-compatible API)
    """
    llm = ChatOpenAI(
        model=LLM_MODEL,
        api_key=LLM_API_KEY,
        base_url=LLM_BASE_URL,
        temperature=0.1,
        max_tokens=4096,  # TÄƒng max_tokens Ä‘á»ƒ trÃ¡nh LLMDidNotFinishException
    )
    return LangchainLLMWrapper(llm)


def create_ragas_embeddings():
    """
    Táº¡o Embedding wrapper cho RAGAS sá»­ dá»¥ng NVIDIA NIM embeddings
    """
    embeddings = OpenAIEmbeddings(
        model="nvidia/nv-embedqa-e5-v5",
        api_key=LLM_API_KEY,
        base_url=LLM_BASE_URL,
    )
    return LangchainEmbeddingsWrapper(embeddings)


def prepare_ragas_dataset(pipeline_results: List[Dict]) -> EvaluationDataset:
    """
    Chuyá»ƒn Ä‘á»•i káº¿t quáº£ tá»« RAG pipeline sang EvaluationDataset (RAGAS 0.4.x)

    RAGAS 0.4.x sá»­ dá»¥ng SingleTurnSample vá»›i cÃ¡c trÆ°á»ng:
    - user_input: cÃ¢u há»i
    - response: cÃ¢u tráº£ lá»i tá»« LLM
    - retrieved_contexts: list cÃ¡c context chunks (List[str])
    - reference: Ä‘Ã¡p Ã¡n mong Ä‘á»£i (ground truth)
    """
    samples = []
    for result in pipeline_results:
        # Láº¥y content tá»« cÃ¡c chunks Ä‘Ã£ retrieve
        chunk_contents = [chunk["content"] for chunk in result["retrieved_chunks"]]

        # Äáº£m báº£o má»—i context lÃ  string
        chunk_contents = [str(c) for c in chunk_contents if c]

        # Äáº£m báº£o reference khÃ´ng rá»—ng (RAGAS cáº§n reference cho ContextRecall)
        reference = result.get("expected_answer", "")
        if not reference or not reference.strip():
            reference = "KhÃ´ng cÃ³ Ä‘Ã¡p Ã¡n tham chiáº¿u."

        sample = SingleTurnSample(
            user_input=str(result["query"]),
            response=str(result["answer"]),
            retrieved_contexts=chunk_contents,
            reference=str(reference),
        )
        samples.append(sample)

    return EvaluationDataset(samples=samples)


def run_ragas_evaluation(pipeline_results: List[Dict]) -> Dict:
    """
    Cháº¡y Ä‘Ã¡nh giÃ¡ RAGAS trÃªn káº¿t quáº£ pipeline

    Returns:
        Dict chá»©a scores cho cÃ¡c metrics
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š Báº®T Äáº¦U ÄÃNH GIÃ Vá»šI RAGAS (v0.4.x)")
    print("=" * 60)

    # Chuáº©n bá»‹ dataset
    print("\nğŸ”„ Äang chuáº©n bá»‹ dataset cho RAGAS...")
    dataset = prepare_ragas_dataset(pipeline_results)
    print(f"âœ… Dataset: {len(dataset)} samples")

    # Táº¡o LLM vÃ  Embeddings cho RAGAS
    print("ğŸ”„ Äang khá»Ÿi táº¡o LLM vÃ  Embeddings cho RAGAS...")
    ragas_llm = create_ragas_llm()
    ragas_embeddings = create_ragas_embeddings()

    # Khá»Ÿi táº¡o metrics (RAGAS 0.4.x dÃ¹ng class instances, truyá»n llm/embeddings khi khá»Ÿi táº¡o)
    metrics = [
        Faithfulness(llm=ragas_llm),
        ResponseRelevancy(llm=ragas_llm, embeddings=ragas_embeddings),
        LLMContextPrecisionWithReference(llm=ragas_llm),
        ContextRecall(llm=ragas_llm),
    ]

    metric_names = [
        "faithfulness",
        "answer_relevancy",
        "context_precision",
        "context_recall",
    ]

    # Cáº¥u hÃ¬nh RunConfig: cháº¡y tuáº§n tá»± Ä‘á»ƒ trÃ¡nh lá»—i vá»›i NVIDIA NIM
    run_config = RunConfig(
        max_workers=1,
        timeout=180,
        max_retries=5,
    )

    print("ğŸ”„ Äang cháº¡y evaluation (cÃ³ thá»ƒ máº¥t vÃ i phÃºt)...")

    # Cháº¡y tá»«ng metric riÃªng láº» Ä‘á»ƒ trÃ¡nh lá»—i má»™t metric áº£nh hÆ°á»Ÿng táº¥t cáº£
    all_scores = {}
    for metric, name in zip(metrics, metric_names):
        print(f"\n  ğŸ”„ Äang Ä‘Ã¡nh giÃ¡: {name}...")
        try:
            result = evaluate(
                dataset=dataset,
                metrics=[metric],
                run_config=run_config,
                raise_exceptions=False,
                batch_size=1,
            )
            # Láº¥y score tá»« result - chuyá»ƒn sang pandas DataFrame
            result_df = result.to_pandas()
            # TÃªn cá»™t trong káº¿t quáº£ = metric.name (thuá»™c tÃ­nh cá»§a metric class)
            col_name = metric.name
            scores_series = result_df[col_name]
            # Lá»c NaN
            valid_scores = scores_series.dropna().tolist()
            if valid_scores:
                avg = sum(valid_scores) / len(valid_scores)
                all_scores[name] = round(avg, 4)
                print(f"  âœ… {name}: {avg:.4f}  ({len(valid_scores)}/{len(scores_series)} samples valid)")
            else:
                all_scores[name] = None
                print(f"  âš ï¸ {name}: KhÃ´ng cÃ³ káº¿t quáº£ há»£p lá»‡")
        except Exception as e:
            all_scores[name] = None
            print(f"  âŒ {name}: Lá»—i - {str(e)}")

    # TÃ­nh average
    valid_scores = [v for v in all_scores.values() if v is not None]
    if valid_scores:
        all_scores["average_score"] = round(sum(valid_scores) / len(valid_scores), 4)
    else:
        all_scores["average_score"] = None

    # In tá»•ng káº¿t
    print("\n" + "=" * 60)
    print("ğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ RAGAS")
    print("=" * 60)
    for name in metric_names:
        score = all_scores.get(name)
        if score is not None:
            print(f"  {name:25s}: {score:.4f}")
        else:
            print(f"  {name:25s}: N/A")
    avg = all_scores.get("average_score")
    if avg is not None:
        print(f"\n  ğŸ“ˆ {'Average Score':25s}: {avg:.4f}")
    else:
        print(f"\n  ğŸ“ˆ {'Average Score':25s}: N/A")
    print("=" * 60)

    return all_scores


def save_evaluation_report(
    pipeline_results: List[Dict],
    ragas_scores: Dict,
    output_dir: str = OUTPUT_DIR,
):
    """
    LÆ°u bÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡ Ä‘áº§y Ä‘á»§
    """
    os.makedirs(output_dir, exist_ok=True)

    # 1. LÆ°u RAGAS scores (JSON)
    scores_path = os.path.join(output_dir, "ragas_scores.json")
    with open(scores_path, 'w', encoding='utf-8') as f:
        json.dump(ragas_scores, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ RAGAS scores Ä‘Ã£ lÆ°u: {scores_path}")

    # 2. LÆ°u káº¿t quáº£ chi tiáº¿t (JSON)
    detail_path = os.path.join(output_dir, "evaluation_details.json")

    serializable_results = []
    for r in pipeline_results:
        sr = {
            "question_index": r.get("question_index"),
            "question_type": r.get("question_type"),
            "query": r.get("query"),
            "case_context": r.get("case_context", ""),
            "answer": r.get("answer"),
            "expected_answer": r.get("expected_answer"),
            "retrieved_chunks": [
                {
                    "chunk_id": c["chunk_id"],
                    "source": c["metadata"]["filename"],
                    "section": c["metadata"].get("section_header", ""),
                    "hybrid_score": c.get("hybrid_score", 0),
                    "bm25_score": c.get("bm25_score", 0),
                    "dense_score": c.get("dense_score", 0),
                    "content": c["content"][:500],
                }
                for c in r.get("retrieved_chunks", [])
            ]
        }
        serializable_results.append(sr)

    report = {
        "ragas_scores": ragas_scores,
        "total_questions": len(pipeline_results),
        "results": serializable_results,
    }

    with open(detail_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Chi tiáº¿t Ä‘Ã£ lÆ°u: {detail_path}")

    # 3. LÆ°u bÃ¡o cÃ¡o text
    report_path = os.path.join(output_dir, "evaluation_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("BÃO CÃO ÄÃNH GIÃ RAG PIPELINE - HYBRID SEARCH\n")
        f.write("=" * 80 + "\n\n")

        f.write("ğŸ“Š RAGAS SCORES:\n")
        for metric, score in ragas_scores.items():
            if score is not None:
                f.write(f"  {metric}: {score:.4f}\n")
            else:
                f.write(f"  {metric}: N/A\n")

        f.write(f"\nğŸ“‹ Tá»”NG Sá» CÃ‚U Há»I: {len(pipeline_results)}\n\n")

        for i, r in enumerate(pipeline_results):
            f.write(f"\n{'â”€' * 70}\n")
            f.write(f"CÃ¢u {i+1}: {r.get('query', '')}\n")
            f.write(f"Loáº¡i: {r.get('question_type', 'N/A')}\n")
            f.write(f"Tráº£ lá»i:\n{r.get('answer', '')}\n")
            f.write(f"ÄÃ¡p Ã¡n mong Ä‘á»£i:\n{r.get('expected_answer', '')}\n")
            f.write(f"Sá»‘ chunks: {len(r.get('retrieved_chunks', []))}\n")

    print(f"ğŸ’¾ BÃ¡o cÃ¡o Ä‘Ã£ lÆ°u: {report_path}")

    # 4. LÆ°u output pipeline
    output_path = os.path.join(output_dir, "pipeline_output.txt")
    with open(output_path, 'w', encoding='utf-8') as f:
        for i, r in enumerate(pipeline_results):
            f.write(f"\n{'=' * 70}\n")
            f.write(f"CÃ‚U Há»I {i+1}: {r.get('query', '')}\n")
            f.write(f"{'=' * 70}\n")
            f.write(f"\nğŸ“ TRáº¢ Lá»œI:\n{r.get('answer', '')}\n")
            f.write(f"\nğŸ“š CHUNKS ÄÃƒ TÃŒM ({len(r.get('retrieved_chunks', []))}):\n")
            for j, c in enumerate(r.get("retrieved_chunks", [])):
                f.write(f"\n  [{j+1}] Source: {c['metadata']['filename']}")
                f.write(f" | Score: {c.get('hybrid_score', 0):.4f}\n")
                f.write(f"  {c['content'][:300]}...\n")
    print(f"ğŸ’¾ Pipeline output Ä‘Ã£ lÆ°u: {output_path}")
