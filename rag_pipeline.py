"""
Module RAG Pipeline: káº¿t há»£p Hybrid Search vá»›i LLM (NVIDIA NIM) 
Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i dá»±a trÃªn cÃ¡c vÄƒn báº£n phÃ¡p luáº­t
"""
import json
from typing import List, Dict, Optional

from openai import OpenAI

from config import (
    LLM_API_KEY, LLM_BASE_URL, LLM_MODEL, LLM_TEMPERATURE,
    DATA_DIR, CHUNK_SIZE, CHUNK_OVERLAP, TOP_K,
    RAG_PROMPT_TEMPLATE, EVAL_FILE
)
from data_processing import load_documents
from chunking import chunk_documents
from embedding import EmbeddingManager
from hybrid_search import HybridSearchEngine


class RAGPipeline:
    """
    Pipeline RAG Ä‘áº§u-Ä‘áº¿n-cuá»‘i:
    1. Xá»­ lÃ½ dá»¯ liá»‡u â†’ Chunking â†’ Embedding â†’ Indexing
    2. Hybrid Search (BM25 + Dense)
    3. Táº¡o cÃ¢u tráº£ lá»i báº±ng LLM (NVIDIA NIM)
    """
    
    def __init__(
        self,
        data_dir: str = DATA_DIR,
        chunk_size: int = CHUNK_SIZE,
        chunk_overlap: int = CHUNK_OVERLAP,
        top_k: int = TOP_K,
        search_method: str = "weighted",
        force_reindex: bool = False,
    ):
        self.data_dir = data_dir
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.top_k = top_k
        self.search_method = search_method
        
        # Khá»Ÿi táº¡o LLM client (NVIDIA NIM - OpenAI compatible)
        self.llm_client = OpenAI(
            api_key=LLM_API_KEY,
            base_url=LLM_BASE_URL,
        )
        
        # Build pipeline
        self._build_pipeline(force_reindex)
    
    def _build_pipeline(self, force_reindex: bool):
        """XÃ¢y dá»±ng toÃ n bá»™ pipeline"""
        print("\n" + "=" * 60)
        print("ğŸš€ KHá»I Táº O RAG PIPELINE")
        print("=" * 60)
        
        # Step 1: Load vÃ  xá»­ lÃ½ dá»¯ liá»‡u
        print("\nğŸ“‚ BÆ°á»›c 1: Äá»c vÃ  xá»­ lÃ½ dá»¯ liá»‡u...")
        self.documents = load_documents(self.data_dir)
        
        # Step 2: Chunking
        print("\nâœ‚ï¸ BÆ°á»›c 2: Chia chunks...")
        self.chunks = chunk_documents(
            self.documents,
            self.chunk_size,
            self.chunk_overlap
        )
        
        # Step 3: Embedding & Indexing
        print("\nğŸ§® BÆ°á»›c 3: Táº¡o embeddings vÃ  indexing...")
        self.embedding_manager = EmbeddingManager()
        self.embedding_manager.index_chunks(self.chunks, force_recreate=force_reindex)
        
        # Step 4: Khá»Ÿi táº¡o Hybrid Search Engine
        print("\nğŸ” BÆ°á»›c 4: Khá»Ÿi táº¡o Hybrid Search Engine...")
        self.search_engine = HybridSearchEngine(
            self.chunks,
            self.embedding_manager,
        )
        
        print("\n" + "=" * 60)
        print("âœ… RAG PIPELINE ÄÃƒ Sáº´N SÃ€NG!")
        print(f"   - Documents: {len(self.documents)}")
        print(f"   - Chunks: {len(self.chunks)}")
        print(f"   - Search method: {self.search_method}")
        print(f"   - Top-K: {self.top_k}")
        print(f"   - LLM: {LLM_MODEL}")
        print("=" * 60)
    
    def retrieve(self, query: str, top_k: Optional[int] = None) -> List[Dict]:
        """
        Truy váº¥n hybrid search
        
        Returns:
            List[Dict]: Top-K chunks liÃªn quan nháº¥t
        """
        k = top_k or self.top_k
        results = self.search_engine.search(query, top_k=k, method=self.search_method)
        return results
    
    def _build_context(self, retrieved_chunks: List[Dict]) -> str:
        """XÃ¢y dá»±ng context tá»« cÃ¡c chunks Ä‘Ã£ retrieve"""
        context_parts = []
        for i, chunk in enumerate(retrieved_chunks):
            source = chunk["metadata"]["filename"]
            section = chunk["metadata"].get("section_header", "")
            content = chunk["content"]
            context_parts.append(
                f"[Äoáº¡n {i+1}] (Nguá»“n: {source}, Má»¥c: {section})\n{content}"
            )
        return "\n\n---\n\n".join(context_parts)
    
    def generate_answer(self, query: str, context: str) -> str:
        """
        Gá»i LLM Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i dá»±a trÃªn context
        """
        prompt = RAG_PROMPT_TEMPLATE.format(
            context=context,
            question=query
        )
        
        try:
            response = self.llm_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "system", "content": "Báº¡n lÃ  trá»£ lÃ½ phÃ¡p luáº­t chuyÃªn nghiá»‡p, tráº£ lá»i chÃ­nh xÃ¡c dá»±a trÃªn vÄƒn báº£n Ä‘Æ°á»£c cung cáº¥p."},
                    {"role": "user", "content": prompt}
                ],
                temperature=LLM_TEMPERATURE,
                max_tokens=1024,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"[Lá»–I LLM] {str(e)}"
    
    def answer(self, query: str, top_k: Optional[int] = None) -> Dict:
        """
        Pipeline hoÃ n chá»‰nh: Query â†’ Retrieve â†’ Generate Answer
        
        Returns:
            Dict chá»©a:
                - query: CÃ¢u há»i
                - retrieved_chunks: CÃ¡c chunk Ä‘Ã£ retrieve
                - context: Context Ä‘Ã£ xÃ¢y dá»±ng
                - answer: CÃ¢u tráº£ lá»i tá»« LLM
        """
        # Step 1: Retrieve
        retrieved_chunks = self.retrieve(query, top_k)
        
        # Step 2: Build context
        context = self._build_context(retrieved_chunks)
        
        # Step 3: Generate answer
        llm_answer = self.generate_answer(query, context)
        
        return {
            "query": query,
            "retrieved_chunks": retrieved_chunks,
            "context": context,
            "answer": llm_answer,
        }
    
    def run_evaluation(self, eval_file: str = EVAL_FILE, top_k: Optional[int] = None) -> List[Dict]:
        """
        Cháº¡y pipeline trÃªn táº¥t cáº£ cÃ¢u há»i tá»« file evaluation
        
        Returns:
            List[Dict]: Káº¿t quáº£ cho má»—i cÃ¢u há»i
        """
        with open(eval_file, 'r', encoding='utf-8') as f:
            eval_data = json.load(f)
        
        results = []
        total = len(eval_data)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ Báº®T Äáº¦U ÄÃNH GIÃ TRÃŠN {total} CÃ‚U Há»I")
        print(f"{'='*80}")
        
        for idx, item in enumerate(eval_data):
            query = item["query"]
            expected_answer = item.get("expected_answer", "")
            question_type = item.get("type", "unknown")
            case_context = item.get("case", "")
            
            # Náº¿u cÃ³ case context, thÃªm vÃ o query
            full_query = query
            if case_context:
                full_query = f"TÃ¬nh huá»‘ng: {case_context}\n\nCÃ¢u há»i: {query}"
            
            print(f"\n--- CÃ¢u há»i {idx+1}/{total} [{question_type}] ---")
            print(f"Q: {query[:100]}...")
            
            # Cháº¡y pipeline
            result = self.answer(full_query, top_k)
            
            # ThÃªm thÃ´ng tin evaluation
            result["expected_answer"] = expected_answer
            result["question_type"] = question_type
            result["case_context"] = case_context
            result["question_index"] = idx + 1
            
            results.append(result)
            
            print(f"A: {result['answer'][:150]}...")
        
        print(f"\n{'='*80}")
        print(f"âœ… ÄÃƒ HOÃ€N THÃ€NH ÄÃNH GIÃ {total} CÃ‚U Há»I")
        print(f"{'='*80}")
        
        return results
    
    def format_results(self, results: List[Dict]) -> str:
        """
        Format káº¿t quáº£ Ä‘áº§u ra theo máº«u yÃªu cáº§u
        """
        output_parts = []
        
        for r in results:
            part = []
            part.append(f"{'='*80}")
            part.append(f"ğŸ“Œ CÃ‚U Há»I {r['question_index']} [{r['question_type']}]")
            part.append(f"{'='*80}")
            
            # 1. CÃ¢u há»i
            part.append(f"\nâ“ CÃ‚U Há»I:")
            part.append(r["query"])
            
            if r.get("case_context"):
                part.append(f"\nğŸ“‹ TÃŒNH HUá»NG:")
                part.append(r["case_context"])
            
            # 2. Top K chunks Ä‘Ã£ truy váº¥n
            part.append(f"\nğŸ” TOP {len(r['retrieved_chunks'])} CHUNKS ÄÃƒ TRUY Váº¤N:")
            for i, chunk in enumerate(r["retrieved_chunks"]):
                part.append(f"\n  --- Chunk {i+1} ---")
                part.append(f"  ID: {chunk['chunk_id']}")
                part.append(f"  Source: {chunk['metadata']['filename']}")
                part.append(f"  Section: {chunk['metadata'].get('section_header', 'N/A')[:80]}")
                part.append(f"  Hybrid Score: {chunk['hybrid_score']:.4f}")
                part.append(f"  BM25 Score: {chunk['bm25_score']:.4f}")
                part.append(f"  Dense Score: {chunk['dense_score']:.4f}")
                part.append(f"  Content: {chunk['content'][:300]}...")
            
            # 3. CÃ¢u tráº£ lá»i LLM
            part.append(f"\nğŸ¤– CÃ‚U TRáº¢ Lá»œI Cá»¦A LLM:")
            part.append(r["answer"])
            
            # 4. ÄÃ¡p Ã¡n mong Ä‘á»£i
            part.append(f"\nâœ… ÄÃP ÃN MONG Äá»¢I:")
            part.append(r["expected_answer"])
            
            part.append("")
            output_parts.append("\n".join(part))
        
        return "\n\n".join(output_parts)


if __name__ == "__main__":
    import os
    from config import OUTPUT_DIR
    
    # Khá»Ÿi táº¡o pipeline
    pipeline = RAGPipeline(force_reindex=True)
    
    # Test vá»›i 1 cÃ¢u há»i
    query = "Theo ThÃ´ng tÆ° 01/1999/TT-BXD, giÃ¡ trá»‹ dá»± toÃ¡n xÃ¢y láº¯p sau thuáº¿ bao gá»“m nhá»¯ng thÃ nh pháº§n nÃ o?"
    result = pipeline.answer(query)
    
    print(f"\n{'='*80}")
    print(f"â“ Query: {result['query']}")
    print(f"\nğŸ” Retrieved {len(result['retrieved_chunks'])} chunks")
    for i, c in enumerate(result['retrieved_chunks']):
        print(f"  [{i+1}] {c['chunk_id']} (score={c['hybrid_score']:.4f}) - {c['metadata']['filename']}")
    print(f"\nğŸ¤– Answer: {result['answer']}")
